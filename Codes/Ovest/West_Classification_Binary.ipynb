{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7744f49-0fb6-4373-a12c-8804f526d13a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Davide\\anaconda3\\envs\\pyspark_env\\Lib\\site-packages\\pyspark\\sql\\context.py:158: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "import pyarrow\n",
    "from pyspark.sql import SQLContext\n",
    "import os\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "conf = pyspark.SparkConf()\\\n",
    "        .setAppName('spark_pipeline')\\\n",
    "        .setMaster('local')\\\n",
    "        .set('spark.driver.memory', '8g')\\\n",
    "        .set('spark.executor.memory', '8g')\\\n",
    "        .set('spark.executor.instances', 4)\\\n",
    "        .set('spark.executor.cores', 4)\\\n",
    "        .set('spark.driver.maxResultSize', '8g')\\\n",
    "        .set('spark.sql.shuffle.partitions', 100)\\\n",
    "        .set('spark.default.parallelism', 200)\\\n",
    "        .set('spark.sql.broadcastTimeout', 36000)\\\n",
    "        .set('spark.kryoserializer.buffer.max', '1024m')\\\n",
    "        .set('spark.sql.execution.arrow.enabled', 'false')\\\n",
    "        .set('spark.dynamicAllocation.enabled', \"False\")\\\n",
    "        .set('spark.port.maxRetries',30) \n",
    "sc = pyspark.SparkContext.getOrCreate(conf)\n",
    "spark = pyspark.sql.SparkSession(sc)\n",
    "sqlContext = SQLContext.getOrCreate(sc)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a91f337-8ce5-48a0-bb95-5d65d1d7227b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyspark.ml.feature import StringIndexer,OneHotEncoder\n",
    "from pyspark.sql import SQLContext, SparkSession, Row\n",
    "from pyspark.mllib.stat import Statistics\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, MinMaxScaler, StandardScaler\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import when, count, isnull\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.functions import when, count, isnull\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics, BinaryClassificationMetrics\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "#spark = SparkSession.Builder().appName('DDAM_Project_west').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b22524b-36a3-495d-9bb6-288bcc043c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.options(inferSchema = True, delimiter = ',', header = True).csv('../../Datasets/West_Incidents_Cleaned.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dde71a29-7d50-49e2-a1d4-2f60892bec5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop('_c0') #devo droppare queste colonne. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3755008-f127-4e03-98a0-7361163a9759",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_class=df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5122dbba-42ac-41f8-96f9-e316f79e7308",
   "metadata": {},
   "source": [
    "## Working_weekend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76c06956-1a96-4cbe-9229-d4256464f4af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Severity', 'Start_Lat', 'Start_Lng', 'Distance_mi', 'Temperature_F', 'Humidity_perc', 'Visibility_mi', 'Wind_Speed_mph', 'Precipitation_in', 'Amenity', 'Bump', 'Crossing', 'Give_Way', 'Junction', 'No_Exit', 'Railway', 'Roundabout', 'Station', 'Stop', 'Traffic_Calming', 'Traffic_Signal', 'month', 'hour', 'Astronomical_Twilight']\n"
     ]
    }
   ],
   "source": [
    "num_col = [item[0] for item in df_class.dtypes if not item[1].startswith('string')]\n",
    "num_col.remove(\"Working_Weekend\")\n",
    "print(num_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552d98f9-ab96-45b2-8a0d-e7734f1d5670",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fb40034-ee09-4df3-aca4-0c075258007e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Definisco funzione le metriche \\ndef print_metrics_and_cf(predictions):\\n    # Select (prediction, true label) and compute test error\\n    evaluator = MulticlassClassificationEvaluator(labelCol=\"Working_Weekend\"\\n                                              , predictionCol=\"prediction\"\\n                                              , metricName=\"accuracy\")\\n\\n    accuracy = evaluator.evaluate(predictions)\\n    print(\"Accuracy :\",accuracy)\\n    print(\"Test Error = %g\" % (1.0 - accuracy))\\n    print(\\'----------------------------\\')\\n    \\n    # metrics\\n    predictionAndLabels = predictions.select(\"prediction\", \"Working_Weekend\").rdd.map(lambda x: (float(x[0]), float(x[1])))\\n    metrics = MulticlassMetrics(predictionAndLabels)\\n    auc=metrics.areaUnderROC\\n    print(\\'----------\\',auc)\\n\\n    list_avg=[]\\n    for i in range(2):\\n        precision = metrics.precision(label=float(i))  \\n        recall = metrics.recall(label=float(i))\\n        f1Score= metrics.fMeasure(label=float(i)) # need .0\\n        print(\"Precision for class \",i+1,\": {:.2%}\".format(precision))\\n        print(\"Recall for class \",i+1,\": {:.2%}\".format(recall))\\n        print(\"avg_F1-Score for class \",i+1,\": {:.2%}\".format(f1Score))\\n        list_avg.append(f1Score)\\n        print(\\'----------------------------\\')\\n    sum_=0\\n    #print(list_avg)\\n    for elem in list_avg:\\n        sum_+=elem\\n\\n    avg_f1=sum_/4\\n    print(\\'----------------------\\')\\n\\n    print(\"avg_F1-Score: {:.2%}\".format(avg_f1))\\n\\n    # Confusion Matrix\\n\\n    labels = [\"0\", \"1\"]# DEVO DEFINIRE working e Weekend \\n    _ = plt.figure(figsize=(7, 7))\\n    sns.heatmap(metrics.confusionMatrix().toArray(),\\n                cmap=\\'viridis\\',\\n                annot=True,fmt=\\'0\\',\\n                cbar=False, \\n                xticklabels=labels, \\n                yticklabels=labels)'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Definisco funzione le metriche \n",
    "def print_metrics_and_cf(predictions):\n",
    "    # Select (prediction, true label) and compute test error\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=\"Working_Weekend\"\n",
    "                                              , predictionCol=\"prediction\"\n",
    "                                              , metricName=\"accuracy\")\n",
    "\n",
    "    accuracy = evaluator.evaluate(predictions)\n",
    "    print(\"Accuracy :\",accuracy)\n",
    "    print(\"Test Error = %g\" % (1.0 - accuracy))\n",
    "    print('----------------------------')\n",
    "    \n",
    "    # metrics\n",
    "    predictionAndLabels = predictions.select(\"prediction\", \"Working_Weekend\").rdd.map(lambda x: (float(x[0]), float(x[1])))\n",
    "    metrics = MulticlassMetrics(predictionAndLabels)\n",
    "    auc=metrics.areaUnderROC\n",
    "    print('----------',auc)\n",
    "\n",
    "    list_avg=[]\n",
    "    for i in range(2):\n",
    "        precision = metrics.precision(label=float(i))  \n",
    "        recall = metrics.recall(label=float(i))\n",
    "        f1Score= metrics.fMeasure(label=float(i)) # need .0\n",
    "        print(\"Precision for class \",i+1,\": {:.2%}\".format(precision))\n",
    "        print(\"Recall for class \",i+1,\": {:.2%}\".format(recall))\n",
    "        print(\"avg_F1-Score for class \",i+1,\": {:.2%}\".format(f1Score))\n",
    "        list_avg.append(f1Score)\n",
    "        print('----------------------------')\n",
    "    sum_=0\n",
    "    #print(list_avg)\n",
    "    for elem in list_avg:\n",
    "        sum_+=elem\n",
    "\n",
    "    avg_f1=sum_/4\n",
    "    print('----------------------')\n",
    "\n",
    "    print(\"avg_F1-Score: {:.2%}\".format(avg_f1))\n",
    "\n",
    "    # Confusion Matrix\n",
    "\n",
    "    labels = [\"0\", \"1\"]# DEVO DEFINIRE working e Weekend \n",
    "    _ = plt.figure(figsize=(7, 7))\n",
    "    sns.heatmap(metrics.confusionMatrix().toArray(),\n",
    "                cmap='viridis',\n",
    "                annot=True,fmt='0',\n",
    "                cbar=False, \n",
    "                xticklabels=labels, \n",
    "                yticklabels=labels)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c90fc1d-63b2-420b-bd10-448adfbc3143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    labels = [\"0\", \"1\"]\\n    _ = plt.figure(figsize=(7, 7))\\n    sns.heatmap(metrics.confusionMatrix().toArray(),\\n                cmap=\\'viridis\\',\\n                annot=True,fmt=\\'0\\',\\n                cbar=False, \\n                xticklabels=labels, \\n                yticklabels=labels)'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def print_metrics_and_cf(predictions):\n",
    "    # Select (prediction, true label) and compute test error\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=\"Working_Weekend\"\n",
    "                                              , predictionCol=\"prediction\"\n",
    "                                              , metricName=\"accuracy\")\n",
    "\n",
    "    accuracy = evaluator.evaluate(predictions)\n",
    "    print(\"Accuracy :\",accuracy)\n",
    "    print(\"Test Error = %g\" % (1.0 - accuracy))\n",
    "    print('----------------------------')\n",
    "\n",
    "    # AUC score \n",
    "    scoreAndLabels = predictions.select('probability','Working_Weekend').rdd.map(lambda row: (float(row['Working_Weekend']),float(row['probability'][1]) ))\n",
    "    metrics = BinaryClassificationMetrics(scoreAndLabels)\n",
    "    auROC = metrics.areaUnderROC\n",
    "    print('Auc_:', auROC,'\\n ----------------------')\n",
    "    \n",
    "    \n",
    "    # metrics\n",
    "    predictionAndLabels = predictions.select(\"prediction\", \"Working_Weekend\").rdd.map(lambda x: (float(x[0]), float(x[1])))\n",
    "    metrics = MulticlassMetrics(predictionAndLabels)\n",
    "\n",
    "    list_avg=[]\n",
    "    for i in range(2):\n",
    "        precision = metrics.precision(label=float(i))  \n",
    "        recall = metrics.recall(label=float(i))\n",
    "        f1Score= metrics.fMeasure(label=float(i)) # need .0\n",
    "        print(\"Precision for class \",i+1,\": {:.2%}\".format(precision))\n",
    "        print(\"Recall for class \",i+1,\": {:.2%}\".format(recall))\n",
    "        print(\"avg_F1-Score for class \",i+1,\": {:.2%}\".format(f1Score))\n",
    "        list_avg.append(f1Score)\n",
    "        print('----------------------------')\n",
    "    sum_=0\n",
    "    #print(list_avg)\n",
    "    for elem in list_avg:\n",
    "        sum_+=elem\n",
    "\n",
    "    avg_f1=sum_/2\n",
    "    print('----------------------')\n",
    "\n",
    "    print(\"avg_F1-Score: {:.2%}\".format(avg_f1))\n",
    "\n",
    "    # Confusion Matrix\n",
    "    #alternativa per pc \n",
    "    print('----------------------')\n",
    "    cf= metrics.confusionMatrix().toArray()\n",
    "    print(cf)\n",
    "    print('----------------------')\n",
    "'''    labels = [\"0\", \"1\"]\n",
    "    _ = plt.figure(figsize=(7, 7))\n",
    "    sns.heatmap(metrics.confusionMatrix().toArray(),\n",
    "                cmap='viridis',\n",
    "                annot=True,fmt='0',\n",
    "                cbar=False, \n",
    "                xticklabels=labels, \n",
    "                yticklabels=labels)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e182f022-a087-4ce2-b654-6ecc8f735dee",
   "metadata": {},
   "source": [
    "### A_Sbilanciato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9795ec90-16c2-42d1-9f69-bff1961a8810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------+---------------+\n",
      "|features                                                                                                 |Working_Weekend|\n",
      "+---------------------------------------------------------------------------------------------------------+---------------+\n",
      "|(24,[0,1,2,3,4,5,6,7,21,22],[2.0,36.980083,-120.04043,0.158,56.0,69.0,10.0,5.0,5.0,5.0])                 |0              |\n",
      "|(24,[0,1,2,3,4,5,6,21,22],[2.0,37.02114701253405,-120.13138890209136,1.899,67.0,47.0,10.0,5.0,8.0])      |0              |\n",
      "|(24,[0,1,2,3,4,5,6,7,21,22,23],[2.0,36.841894,-119.983215,0.644,60.0,39.0,10.0,6.0,5.0,23.0,1.0])        |0              |\n",
      "|(24,[0,1,2,3,4,5,6,7,21,22,23],[2.0,36.851273,-119.992134,0.502,60.0,39.0,10.0,6.0,5.0,23.0,1.0])        |0              |\n",
      "|(24,[0,1,2,3,4,5,6,7,18,21,22,23],[2.0,36.923019,-119.946995,0.409,73.0,23.0,10.0,8.0,1.0,5.0,22.0,1.0]) |1              |\n",
      "|(24,[0,1,2,3,4,5,6,7,18,21,22,23],[2.0,36.923019,-119.946995,0.268,73.0,23.0,10.0,8.0,1.0,5.0,22.0,1.0]) |1              |\n",
      "|(24,[0,1,2,3,4,5,6,7,21,22,23],[2.0,36.923045,-119.893387,0.051,73.0,23.0,10.0,8.0,5.0,22.0,1.0])        |1              |\n",
      "|(24,[0,1,2,3,4,5,6,7,21,22,23],[2.0,36.923064,-119.951844,0.949,73.0,23.0,10.0,8.0,5.0,23.0,1.0])        |1              |\n",
      "|(24,[0,1,2,3,4,5,6,21,22],[2.0,36.996419,-120.166106,0.547,73.0,34.0,10.0,5.0,9.0])                      |0              |\n",
      "|(24,[0,1,2,3,4,5,6,7,21,22],[2.0,36.920875,-119.793922,0.192,73.0,46.0,10.0,15.0,5.0,12.0])              |1              |\n",
      "|(24,[0,1,2,3,4,5,6,7,21,22],[2.0,36.914062,-119.912368,0.619,73.0,30.0,10.0,5.0,5.0,14.0])               |0              |\n",
      "|(24,[0,1,2,3,4,5,6,7,21,22],[2.0,36.919122,-119.793844,0.365,73.0,46.0,10.0,15.0,5.0,12.0])              |1              |\n",
      "|(24,[0,1,2,3,4,5,6,7,21,22],[2.0,36.90162,-119.792782,0.3,87.0,20.0,10.0,10.0,5.0,20.0])                 |0              |\n",
      "|(24,[0,1,2,3,4,5,6,7,21,22],[2.0,37.002281,-120.102124,0.237,87.0,18.0,10.0,6.0,5.0,17.0])               |0              |\n",
      "|(24,[0,1,2,3,4,5,6,7,21,22],[2.0,37.04030871373634,-120.15617251338884,1.836,87.0,20.0,1.0,9.0,5.0,14.0])|0              |\n",
      "|(24,[0,1,2,3,4,5,6,7,21,22],[2.0,36.952609,-120.02854,0.371,94.0,22.0,10.0,17.0,5.0,16.0])               |1              |\n",
      "|(24,[0,1,2,3,4,5,6,7,21,22],[2.0,36.923683,-120.165296,0.0069999999999999,94.0,23.0,10.0,17.0,5.0,16.0]) |1              |\n",
      "|(24,[0,1,2,3,4,5,6,7,21,22],[2.0,36.952587,-120.022018,0.36,94.0,22.0,10.0,17.0,5.0,16.0])               |1              |\n",
      "|(24,[0,1,2,4,5,6,7,21,22],[2.0,37.020741,-120.131531,79.0,40.0,10.0,7.0,5.0,13.0])                       |0              |\n",
      "|(24,[0,1,2,4,5,6,7,21,22],[2.0,36.91114,-120.007713,79.0,40.0,10.0,7.0,5.0,13.0])                        |0              |\n",
      "+---------------------------------------------------------------------------------------------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assembler = VectorAssembler(inputCols=num_col, outputCol=\"features\")\n",
    "\n",
    "output_dataset = assembler.transform(df_class)\n",
    "\n",
    "classificationData = output_dataset.select(\"features\", \"Working_Weekend\")\n",
    "\n",
    "classificationData.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2dd695c-7b22-4c2a-a4af-511fd83fb019",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split in train and test \n",
    "(trainingData, testData) = classificationData.randomSplit([0.7, 0.3],seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59acd011-18f9-4f72-bfb6-5ed243031b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------+\n",
      "|Working_Weekend| Count|\n",
      "+---------------+------+\n",
      "|              0|205925|\n",
      "|              1| 61513|\n",
      "+---------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "g=trainingData.groupBy('Working_Weekend').agg(F.count('Working_Weekend').alias('Count'))\n",
    "g=g.sort('Working_Weekend')\n",
    "g.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0aa4861a-7dac-45ec-bda0-e1e9b267f081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the count of instances\n",
    "g_p=g.toPandas()\n",
    "n_0=g_p.iloc[0,1]\n",
    "n_1=g_p.iloc[1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d66f664c-800e-40ce-840d-9edce9394f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+\n",
      "|            features|Working_Weekend|\n",
      "+--------------------+---------------+\n",
      "|(24,[0,1,2,3,4,5,...|              0|\n",
      "|(24,[0,1,2,3,4,5,...|              0|\n",
      "|(24,[0,1,2,3,4,5,...|              0|\n",
      "|(24,[0,1,2,3,4,5,...|              0|\n",
      "|(24,[0,1,2,3,4,5,...|              0|\n",
      "|(24,[0,1,2,3,4,5,...|              1|\n",
      "|(24,[0,1,2,3,4,5,...|              0|\n",
      "|(24,[0,1,2,3,4,5,...|              0|\n",
      "|(24,[0,1,2,3,4,5,...|              0|\n",
      "|(24,[0,1,2,3,4,5,...|              0|\n",
      "|(24,[0,1,2,3,4,5,...|              1|\n",
      "|(24,[0,1,2,3,4,5,...|              1|\n",
      "|(24,[0,1,2,3,4,5,...|              0|\n",
      "|(24,[0,1,2,3,4,5,...|              0|\n",
      "|(24,[0,1,2,3,4,5,...|              0|\n",
      "|(24,[0,1,2,3,4,5,...|              0|\n",
      "|(24,[0,1,2,3,4,5,...|              0|\n",
      "|(24,[0,1,2,3,4,5,...|              0|\n",
      "|(24,[0,1,2,3,4,5,...|              0|\n",
      "|(24,[0,1,2,3,4,5,...|              0|\n",
      "+--------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainingData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa22de95-2b04-400a-a716-563c6aee8c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+\n",
      "|            features|Working_Weekend|\n",
      "+--------------------+---------------+\n",
      "|(24,[0,1,2,3,4,5,...|              0|\n",
      "|(24,[0,1,2,3,4,5,...|              0|\n",
      "|(24,[0,1,2,3,4,5,...|              0|\n",
      "|(24,[0,1,2,3,4,5,...|              0|\n",
      "|(24,[0,1,2,3,4,5,...|              0|\n",
      "|(24,[0,1,2,3,4,5,...|              0|\n",
      "|(24,[0,1,2,3,4,5,...|              0|\n",
      "|(24,[0,1,2,3,4,5,...|              0|\n",
      "|(24,[0,1,2,3,4,5,...|              0|\n",
      "|(24,[0,1,2,3,4,5,...|              0|\n",
      "|(24,[0,1,2,3,4,5,...|              1|\n",
      "|(24,[0,1,2,3,4,5,...|              1|\n",
      "|(24,[0,1,2,3,4,5,...|              0|\n",
      "|(24,[0,1,2,3,4,5,...|              0|\n",
      "|(24,[0,1,2,3,4,5,...|              0|\n",
      "|(24,[0,1,2,3,4,5,...|              0|\n",
      "|(24,[0,1,2,3,4,5,...|              0|\n",
      "|(24,[0,1,2,3,4,5,...|              0|\n",
      "|(24,[0,1,2,3,4,5,...|              0|\n",
      "|(24,[0,1,2,3,4,5,...|              0|\n",
      "+--------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testData.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58633b0-75f7-4674-86d6-6ed7ee289611",
   "metadata": {},
   "source": [
    "### A_ DT base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b058a8b-e522-4cc9-acf8-eac1707dda82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier, RandomForestClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(labelCol=\"Working_Weekend\", featuresCol=\"features\")\n",
    "\n",
    "dt = dt.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f7118ed-6b1e-458e-affc-9304fee53d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+------------------+--------------------+----------+\n",
      "|            features|Working_Weekend|     rawPrediction|         probability|prediction|\n",
      "+--------------------+---------------+------------------+--------------------+----------+\n",
      "|(24,[0,1,2,3,4,5,...|              0|[128036.0,38947.0]|[0.76676068821377...|       0.0|\n",
      "|(24,[0,1,2,3,4,5,...|              0|    [7392.0,739.0]|[0.90911327020046...|       0.0|\n",
      "|(24,[0,1,2,3,4,5,...|              0|[128036.0,38947.0]|[0.76676068821377...|       0.0|\n",
      "|(24,[0,1,2,3,4,5,...|              0|   [11831.0,509.0]|[0.95875202593192...|       0.0|\n",
      "|(24,[0,1,2,3,4,5,...|              0|[128036.0,38947.0]|[0.76676068821377...|       0.0|\n",
      "|(24,[0,1,2,3,4,5,...|              0|   [11831.0,509.0]|[0.95875202593192...|       0.0|\n",
      "|(24,[0,1,2,3,4,5,...|              0|  [27415.0,4613.0]|[0.85596977644561...|       0.0|\n",
      "|(24,[0,1,2,3,4,5,...|              0|[128036.0,38947.0]|[0.76676068821377...|       0.0|\n",
      "|(24,[0,1,2,3,4,5,...|              0|[128036.0,38947.0]|[0.76676068821377...|       0.0|\n",
      "|(24,[0,1,2,3,4,5,...|              0|   [3900.0,1222.0]|[0.76142131979695...|       0.0|\n",
      "|(24,[0,1,2,3,4,5,...|              1|   [7615.0,6261.0]|[0.54878927644854...|       0.0|\n",
      "|(24,[0,1,2,3,4,5,...|              1|   [7615.0,6261.0]|[0.54878927644854...|       0.0|\n",
      "|(24,[0,1,2,3,4,5,...|              0|[128036.0,38947.0]|[0.76676068821377...|       0.0|\n",
      "|(24,[0,1,2,3,4,5,...|              0|  [27415.0,4613.0]|[0.85596977644561...|       0.0|\n",
      "|(24,[0,1,2,3,4,5,...|              0|[128036.0,38947.0]|[0.76676068821377...|       0.0|\n",
      "|(24,[0,1,2,3,4,5,...|              0|  [27415.0,4613.0]|[0.85596977644561...|       0.0|\n",
      "|(24,[0,1,2,3,4,5,...|              0|   [11831.0,509.0]|[0.95875202593192...|       0.0|\n",
      "|(24,[0,1,2,3,4,5,...|              0|   [11831.0,509.0]|[0.95875202593192...|       0.0|\n",
      "|(24,[0,1,2,3,4,5,...|              0|[128036.0,38947.0]|[0.76676068821377...|       0.0|\n",
      "|(24,[0,1,2,3,4,5,...|              0|[128036.0,38947.0]|[0.76676068821377...|       0.0|\n",
      "+--------------------+---------------+------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions.\n",
    "predictions = dt.transform(testData)\n",
    "\n",
    "predictions.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d13d5dd2-01f7-4265-99ae-3b9f8b7bcf72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.7709629031833612\n",
      "Test Error = 0.229037\n",
      "----------------------------\n",
      "Auc_: 0.6820996993299244 \n",
      " ----------------------\n",
      "Precision for class  1 : 77.18%\n",
      "Recall for class  1 : 99.77%\n",
      "avg_F1-Score for class  1 : 87.03%\n",
      "----------------------------\n",
      "Precision for class  2 : 59.24%\n",
      "Recall for class  2 : 1.14%\n",
      "avg_F1-Score for class  2 : 2.23%\n",
      "----------------------------\n",
      "----------------------\n",
      "avg_F1-Score: 44.63%\n",
      "----------------------\n",
      "[[87591.   205.]\n",
      " [25905.   298.]]\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "print_metrics_and_cf(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "980ead2b-217a-4e1f-9504-84a0b6509914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(24, {0: 0.0173, 1: 0.0181, 3: 0.1032, 8: 0.0271, 21: 0.0204, 22: 0.381, 23: 0.4329})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.featureImportances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0d0dcc-2f14-49e1-9f21-5733fbb24ff7",
   "metadata": {},
   "source": [
    "### A___ DT testo configurazioni diverse di parametri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9687c330-9c41-4c1a-8da4-c73dbec137b6",
   "metadata": {},
   "source": [
    "#### a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a2e6a04-f68e-41d4-a220-d4815090a89d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.7876735760840007\n",
      "Test Error = 0.212326\n",
      "----------------------------\n",
      "Auc_: 0.6938508275341404 \n",
      " ----------------------\n",
      "Precision for class  1 : 82.14%\n",
      "Recall for class  1 : 92.56%\n",
      "avg_F1-Score for class  1 : 87.04%\n",
      "----------------------------\n",
      "Precision for class  2 : 56.63%\n",
      "Recall for class  2 : 32.55%\n",
      "avg_F1-Score for class  2 : 41.34%\n",
      "----------------------------\n",
      "----------------------\n",
      "avg_F1-Score: 64.19%\n",
      "----------------------\n",
      "[[81266.  6530.]\n",
      " [17675.  8528.]]\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier, RandomForestClassifier\n",
    "\n",
    "dt_param1 = DecisionTreeClassifier(labelCol=\"Working_Weekend\", featuresCol=\"features\",\n",
    "                                  maxDepth=20,\n",
    "                                  maxBins=32)\n",
    "                                   \n",
    "\n",
    "dt_param1 = dt_param1.fit(trainingData)\n",
    "predictions_param1 = dt_param1.transform(testData)\n",
    "print_metrics_and_cf(predictions_param1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f8fa9c1d-370f-4a58-b185-9457065b4be1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(24, {0: 0.0138, 1: 0.1427, 2: 0.1205, 3: 0.101, 4: 0.1299, 5: 0.1168, 6: 0.0416, 7: 0.0832, 8: 0.019, 9: 0.0016, 10: 0.0004, 11: 0.0046, 12: 0.0006, 13: 0.0058, 14: 0.0004, 15: 0.0022, 17: 0.0022, 18: 0.0041, 19: 0.0001, 20: 0.0032, 21: 0.0815, 22: 0.0869, 23: 0.0379})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_param1.featureImportances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20c42af-d1ce-44eb-9ffc-7b9de161cccf",
   "metadata": {},
   "source": [
    "#### b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf5a43b7-644b-4c19-97f7-288d6064a59f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n=trainingData.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fe8e9029-9512-4b4a-a0bc-3596b1e9e210",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.7891735892420109\n",
      "Test Error = 0.210826\n",
      "----------------------------\n",
      "Auc_: 0.7012884398417275 \n",
      " ----------------------\n",
      "Precision for class  1 : 85.99%\n",
      "Recall for class  1 : 86.76%\n",
      "avg_F1-Score for class  1 : 86.37%\n",
      "----------------------------\n",
      "Precision for class  2 : 54.27%\n",
      "Recall for class  2 : 52.64%\n",
      "avg_F1-Score for class  2 : 53.44%\n",
      "----------------------------\n",
      "----------------------\n",
      "avg_F1-Score: 69.91%\n",
      "----------------------\n",
      "[[76171. 11625.]\n",
      " [12409. 13794.]]\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier, RandomForestClassifier\n",
    "\n",
    "dt_param1 = DecisionTreeClassifier(labelCol=\"Working_Weekend\", featuresCol=\"features\",\n",
    "                                  maxDepth=30,\n",
    "                                  maxBins=32,\n",
    "                                  #minInstancesPerNode=round(0.01*n)\n",
    "                                  )\n",
    "                                   \n",
    "\n",
    "dt_param1 = dt_param1.fit(trainingData)\n",
    "predictions_param1 = dt_param1.transform(testData)\n",
    "print_metrics_and_cf(predictions_param1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "697867aa-c30b-473a-83e2-f94d72d58294",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(24, {0: 0.0103, 1: 0.1587, 2: 0.1342, 3: 0.1159, 4: 0.1375, 5: 0.1269, 6: 0.032, 7: 0.0896, 8: 0.0104, 9: 0.0019, 10: 0.0003, 11: 0.0054, 12: 0.0005, 13: 0.0075, 14: 0.0004, 15: 0.0017, 17: 0.0028, 18: 0.0055, 19: 0.0001, 20: 0.0046, 21: 0.0746, 22: 0.0601, 23: 0.0191})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_param1.featureImportances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2493d71f-55cc-42dd-a78a-d3f660d12860",
   "metadata": {},
   "source": [
    "## SOPRA RISULTATO MIGLIORE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c524edc6-7ada-4ca3-8be7-a59f958d2446",
   "metadata": {},
   "source": [
    "#### c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dc770ee6-3e86-4fb0-ac7e-74d2157647f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------+\n",
      "|Working_Weekend| Count|\n",
      "+---------------+------+\n",
      "|              1| 61513|\n",
      "|              0|205925|\n",
      "+---------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "g=trainingData.groupBy('Working_Weekend').agg(F.count('Working_Weekend').alias('Count'))\n",
    "g.sort('Count').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b9e70f3a-277e-43cf-8c66-aabef80e2c4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#trainingData_w = trainingData.withColumn(\"weights1\", lit(1)) # questo Ã¨ con tutti pesi 1  \n",
    "# Creiamo la colonna 'weights' basata sulla frequenza delle classi\n",
    "#trainingData_w\n",
    "trainingData_w = trainingData.withColumn(\"weights2\", when(trainingData[\"Working_Weekend\"] == 0, 1.0)\n",
    "    .when(trainingData[\"Working_Weekend\"] == 1, 267020 / 61237)  # Calcolo del peso per la classe 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ee9266-8ca8-494f-b801-906db3ebd532",
   "metadata": {},
   "source": [
    "#### c_jj) provo la lista dei pesi weights2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1c1d70d4-2981-4187-8cd7-31a86d4b1f1f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.7684277932262563\n",
      "Test Error = 0.231572\n",
      "----------------------------\n",
      "Auc_: 0.6829655813343057 \n",
      " ----------------------\n",
      "Precision for class  1 : 86.91%\n",
      "Recall for class  1 : 82.33%\n",
      "avg_F1-Score for class  1 : 84.56%\n",
      "----------------------------\n",
      "Precision for class  2 : 49.68%\n",
      "Recall for class  2 : 58.46%\n",
      "avg_F1-Score for class  2 : 53.71%\n",
      "----------------------------\n",
      "----------------------\n",
      "avg_F1-Score: 69.14%\n",
      "----------------------\n",
      "[[72283. 15513.]\n",
      " [10886. 15317.]]\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier, RandomForestClassifier\n",
    "\n",
    "dt_param1 = DecisionTreeClassifier(labelCol=\"Working_Weekend\", featuresCol=\"features\",\n",
    "                                  maxDepth=30,\n",
    "                                  maxBins=32,\n",
    "                                  #Thresholds=[0.60,0.40]\n",
    "                                weightCol='weights2'  \n",
    "                                  )\n",
    "                                   \n",
    "\n",
    "dt_param1 = dt_param1.fit(trainingData_w)\n",
    "predictions_param1 = dt_param1.transform(testData)\n",
    "print_metrics_and_cf(predictions_param1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "37501995-de99-48bc-815a-40178074e614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(24, {0: 0.0185, 1: 0.1397, 2: 0.1252, 3: 0.1154, 4: 0.1325, 5: 0.1273, 6: 0.0324, 7: 0.0915, 8: 0.008, 9: 0.0018, 10: 0.0001, 11: 0.006, 12: 0.0004, 13: 0.0086, 14: 0.0004, 15: 0.0013, 17: 0.0027, 18: 0.0048, 19: 0.0001, 20: 0.0057, 21: 0.0771, 22: 0.0777, 23: 0.0228})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_param1.featureImportances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fba9459-7322-4d51-bd05-32d0d5308dac",
   "metadata": {},
   "source": [
    "### B_Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "026e146b-9674-4303-8fc8-2fdbf8f54b95",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.7701471065535662\n",
      "Test Error = 0.229853\n",
      "----------------------------\n",
      "Auc_: 0.0 \n",
      " ----------------------\n",
      "Precision for class  1 : 77.01%\n",
      "Recall for class  1 : 100.00%\n",
      "avg_F1-Score for class  1 : 87.02%\n",
      "----------------------------\n",
      "Precision for class  2 : 0.00%\n",
      "Recall for class  2 : 0.00%\n",
      "avg_F1-Score for class  2 : 0.00%\n",
      "----------------------------\n",
      "----------------------\n",
      "avg_F1-Score: 43.51%\n",
      "----------------------\n",
      "[[87796.     0.]\n",
      " [26203.     0.]]\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "# Train a RandomForest model.\n",
    "rf = RandomForestClassifier(labelCol=\"Working_Weekend\", featuresCol=\"features\", numTrees=10)\n",
    "\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "model = rf.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "print_metrics_and_cf(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4494e276-fff5-498a-b1ea-3b0287dd13f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(24, {0: 0.1005, 1: 0.0204, 2: 0.0028, 3: 0.1468, 4: 0.0117, 5: 0.0038, 6: 0.0054, 7: 0.0021, 8: 0.0099, 14: 0.0001, 15: 0.0002, 17: 0.0, 18: 0.0006, 20: 0.0003, 21: 0.0132, 22: 0.4612, 23: 0.2211})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.featureImportances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015d59e9-7adc-47d4-8ea0-9e0cce29f60f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### B_i) TUNING RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6409444e-9df8-43e0-88c7-7171a9069567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.7965859349643418\n",
      "Test Error = 0.203414\n",
      "----------------------------\n",
      "Auc_: 0.801283846214784 \n",
      " ----------------------\n",
      "Precision for class  1 : 79.61%\n",
      "Recall for class  1 : 98.92%\n",
      "avg_F1-Score for class  1 : 88.22%\n",
      "----------------------------\n",
      "Precision for class  2 : 80.64%\n",
      "Recall for class  2 : 15.14%\n",
      "avg_F1-Score for class  2 : 25.49%\n",
      "----------------------------\n",
      "----------------------\n",
      "avg_F1-Score: 56.85%\n",
      "----------------------\n",
      "[[86844.   952.]\n",
      " [22237.  3966.]]\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(labelCol=\"Working_Weekend\", featuresCol=\"features\", numTrees=20,maxDepth=20, seed=10\n",
    "                           )\n",
    "# Train model.  This also runs the indexers.\n",
    "model = rf.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "print_metrics_and_cf(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bfddc94b-dba8-4948-a52a-8757f78f3dcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(24, {0: 0.0214, 1: 0.1028, 2: 0.098, 3: 0.0979, 4: 0.1018, 5: 0.1024, 6: 0.0456, 7: 0.0763, 8: 0.0251, 9: 0.0028, 10: 0.0003, 11: 0.0068, 12: 0.0007, 13: 0.0092, 14: 0.0009, 15: 0.003, 17: 0.0047, 18: 0.0064, 19: 0.0007, 20: 0.0074, 21: 0.0895, 22: 0.1527, 23: 0.0437})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.featureImportances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990e30a7-2007-444e-825b-4c21d2e7030e",
   "metadata": {},
   "source": [
    "### C_NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3726bc23-46a3-4ea3-a18b-be2c4fb977c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.options(inferSchema = True, delimiter = ',', header = True).csv('WC_NoMissing_OK.csv') \n",
    "df=df.drop('Wind_Chill_F','Pressure_in','_c0') #devo droppare queste colonne. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0f9e6d2d-49b5-4f19-98ca-cecbb92898f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop=['Start_Time','City','County','State','Wind_Direction','day_of_the_week','season']\n",
    "df_class=df.drop(*to_drop)\n",
    "colonne_booleane = ['Amenity', 'Bump', 'Crossing', 'Give_Way', 'Junction', 'No_Exit', 'Railway', 'Roundabout', 'Station', 'Stop', 'Traffic_Calming', 'Traffic_Signal']\n",
    "\n",
    "# Converti le colonne booleane in numeriche\n",
    "for col_name in colonne_booleane:\n",
    "    df_class = df_class.withColumn(col_name, col(col_name).cast(\"int\"))\n",
    "\n",
    "\n",
    "df_class = df_class.withColumn(\"Astronomical_Twilight\", when(df_class[\"Astronomical_Twilight\"] == 'Day', 0).otherwise(1))\n",
    "df_class = df_class.withColumn(\"Working_Weekend\", when(df_class[\"Working_Weekend\"] == 'WorkingDay', 0).otherwise(1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "indexer = StringIndexer(inputCol='Weather_Condition', outputCol='class_numeric')\n",
    "indexer_fitted = indexer.fit(df_class)\n",
    "df_indexed = indexer_fitted.transform(df_class)\n",
    "\n",
    "\n",
    "encoder = OneHotEncoder(inputCols=['class_numeric'], outputCols=['class_onehot'],dropLast=False)\n",
    "df_onehot = encoder.fit(df_indexed).transform(df_indexed)\n",
    "\n",
    "\n",
    "df_col_onehot = df_onehot.select('*', vector_to_array('class_onehot').alias('col_onehot'))\n",
    "\n",
    "num_categories = len(df_col_onehot.first()['col_onehot'])   # 3\n",
    "cols_expanded = [(F.col('col_onehot')[i].alias(f'{indexer_fitted.labels[i]}')) for i in range(num_categories)]\n",
    "df_cols_onehot = df_col_onehot.select('Severity',\n",
    " 'Start_Lat',\n",
    " 'Start_Lng',\n",
    " 'Distance_mi',\n",
    " 'Temperature_F',\n",
    " 'Humidity_perc',\n",
    " 'Visibility_mi',\n",
    " 'Wind_Speed_mph',\n",
    " 'Precipitation_in',\n",
    " 'Amenity',\n",
    " 'Bump',\n",
    " 'Crossing',\n",
    " 'Give_Way',\n",
    " 'Junction',\n",
    " 'No_Exit',\n",
    " 'Railway',\n",
    " 'Roundabout',\n",
    " 'Station',\n",
    " 'Stop',\n",
    " 'Traffic_Calming',\n",
    " 'Traffic_Signal',\n",
    " 'month',\n",
    " 'hour',\n",
    " 'Working_Weekend',\n",
    " 'Astronomical_Twilight', *cols_expanded)\n",
    "\n",
    "\n",
    "num_col = [item[0] for item in df_cols_onehot.dtypes if not item[1].startswith('string')]\n",
    "num_col.remove(\"Severity\")\n",
    "\n",
    "num_col_ww=[item[0] for item in df_cols_onehot.dtypes if not item[1].startswith('string')]\n",
    "num_col_ww.remove(\"Working_Weekend\")\n",
    "\n",
    "df_MLP= df_cols_onehot.withColumn(\"Severity\", col(\"Severity\") - 1)\n",
    "#df_MLP.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b0191978-17df-4cae-819c-da0a4b13e784",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_MLP_binary=df_MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "678c67fc-8eb0-4e61-b60f-a9c2c55ce775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------+---------------+\n",
      "|features                                                                                                          |Working_Weekend|\n",
      "+------------------------------------------------------------------------------------------------------------------+---------------+\n",
      "|(29,[0,1,2,3,4,5,6,7,18,21,22,24],[1.0,39.324703,-123.802754,0.126,68.0,35.0,10.0,10.0,1.0,5.0,11.0,1.0])         |0              |\n",
      "|(29,[0,1,2,3,4,5,6,7,21,22,24],[1.0,38.991387,-123.353783,0.513,73.0,27.0,10.0,12.0,5.0,16.0,1.0])                |0              |\n",
      "|(29,[0,1,2,3,4,5,6,7,21,22,24],[1.0,38.98485,-123.350998,0.935,73.0,27.0,10.0,12.0,5.0,16.0,1.0])                 |0              |\n",
      "|(29,[0,1,2,3,4,5,6,7,21,22,24],[1.0,39.771345,-123.248122,0.64,83.0,18.0,10.0,7.0,5.0,16.0,1.0])                  |0              |\n",
      "|(29,[0,1,2,3,4,5,6,7,21,22,24],[1.0,39.814453,-123.065282,1.267,50.0,54.0,10.0,9.0,5.0,20.0,1.0])                 |0              |\n",
      "|(29,[0,1,2,3,4,5,6,7,21,22,24],[1.0,39.807921,-123.248066,0.127,50.0,54.0,10.0,9.0,5.0,20.0,1.0])                 |0              |\n",
      "|(29,[0,1,2,3,4,5,6,7,21,22,24],[1.0,39.795178,-123.25757,0.171,57.0,47.0,10.0,9.0,5.0,21.0,1.0])                  |0              |\n",
      "|(29,[0,1,2,3,4,5,6,7,21,22,24],[1.0,39.795175,-123.260782,0.171,57.0,47.0,10.0,9.0,5.0,21.0,1.0])                 |0              |\n",
      "|(29,[0,1,2,3,4,5,6,7,21,22,24],[1.0,39.795175,-123.260782,0.093,57.0,47.0,10.0,9.0,5.0,21.0,1.0])                 |0              |\n",
      "|(29,[0,1,2,3,4,5,6,7,21,22,24],[1.0,39.39338400000001,-123.77841399999998,0.045,67.0,28.0,10.0,20.0,5.0,18.0,1.0])|0              |\n",
      "|(29,[0,1,2,3,4,5,6,21,22,26],[1.0,39.551777,-123.763059,0.163,60.0,83.0,4.0,5.0,17.0,1.0])                        |1              |\n",
      "|(29,[0,1,2,3,4,5,6,21,22,26],[1.0,39.445415,-123.718242,0.399,60.0,83.0,4.0,5.0,17.0,1.0])                        |1              |\n",
      "|(29,[0,1,2,3,4,5,6,7,21,22,24],[1.0,39.567433,-123.765727,0.028,73.0,29.0,10.0,8.0,5.0,10.0,1.0])                 |0              |\n",
      "|(29,[0,1,2,3,4,5,6,7,21,22,24],[1.0,39.515163,-123.76336,0.108,61.0,38.0,10.0,3.0,5.0,22.0,1.0])                  |0              |\n",
      "|(29,[0,1,2,3,4,5,6,7,21,22,24],[1.0,39.515163,-123.76336,0.1,61.0,38.0,10.0,3.0,5.0,22.0,1.0])                    |0              |\n",
      "|(29,[0,1,2,3,4,5,6,7,21,22,24],[1.0,39.58495,-123.772044,0.573,48.0,71.0,10.0,5.0,5.0,6.0,1.0])                   |1              |\n",
      "|(29,[0,1,2,3,4,5,6,7,21,22,25],[1.0,39.451804,-123.804503,0.09,54.0,55.0,10.0,5.0,5.0,9.0,1.0])                   |0              |\n",
      "|(29,[0,1,2,3,4,5,6,7,21,22,23,24],[1.0,39.470287,-123.799238,0.026,52.0,45.0,10.0,3.0,5.0,22.0,1.0,1.0])          |0              |\n",
      "|(29,[0,1,2,3,4,5,6,7,21,22,23,24],[1.0,39.470287,-123.799238,0.338,52.0,45.0,10.0,3.0,5.0,22.0,1.0,1.0])          |0              |\n",
      "|(29,[0,1,2,3,4,5,6,7,21,22,24],[1.0,39.37897,-123.797669,0.157,76.0,36.0,10.0,10.0,5.0,15.0,1.0])                 |0              |\n",
      "+------------------------------------------------------------------------------------------------------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assembler_NN = VectorAssembler(inputCols=num_col_ww, outputCol=\"features\")\n",
    "\n",
    "output_dataset_NN = assembler_NN.transform(df_MLP_binary)\n",
    "\n",
    "classificationData_NN = output_dataset_NN.select(\"features\", \"Working_Weekend\")\n",
    "\n",
    "classificationData_NN.show(truncate=False)\n",
    "(trainingData_nn, testData_nn) = classificationData_NN.randomSplit([0.7, 0.3],seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4a0a4c3a-c2c0-40d0-a2f1-7d978ebf1e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.7686605652147309\n",
      "Test Error = 0.231339\n",
      "----------------------------\n",
      "Auc_: 0.0 \n",
      " ----------------------\n",
      "Precision for class  1 : 76.87%\n",
      "Recall for class  1 : 100.00%\n",
      "avg_F1-Score for class  1 : 86.92%\n",
      "----------------------------\n",
      "Precision for class  2 : 0.00%\n",
      "Recall for class  2 : 0.00%\n",
      "avg_F1-Score for class  2 : 0.00%\n",
      "----------------------------\n",
      "----------------------\n",
      "avg_F1-Score: 43.46%\n",
      "----------------------\n",
      "[[87934.     0.]\n",
      " [26465.     0.]]\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "# MLP Layers  -> output layer must have the same number of units of the Working_Weekend classes\n",
    "\n",
    "layers = [len(num_col_ww),16,8,4,2]\n",
    "\n",
    "# Create the Multilayer Perceptron Classifier and set its parameters\n",
    "trainer = MultilayerPerceptronClassifier(\n",
    "    layers=layers,\n",
    "    labelCol=\"Working_Weekend\",\n",
    "    featuresCol=\"features\",\n",
    "    maxIter=50, #!!!!!!!!!!!!!  \n",
    "    tol=1e-6,\n",
    "    seed=None,\n",
    "    blockSize=32,\n",
    "    stepSize=0.03,  \n",
    "    solver=\"l-bfgs\",\n",
    "    initialWeights=None,\n",
    "    probabilityCol=\"probability\",\n",
    "    rawPredictionCol=\"rawPrediction\"\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model = trainer.fit(trainingData_nn)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions= model.transform(testData_nn)\n",
    "\n",
    "print_metrics_and_cf(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "47b5d889-c204-4376-9076-91ff269903c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#As we can see from the results accuracy is very high because the model predict only the labels 1,\n",
    "#cause of the unbalanced data. Now we have to sample the training set trying the define a model well trained \n",
    "#and that can predict each labels on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d5fdab-234f-45a1-b8f9-40160869655c",
   "metadata": {},
   "source": [
    "## UNDERSAMPLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8d31e90a-002b-474f-8996-5aa98e68e69d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_und = trainingData.sampleBy('Working_Weekend', fractions={0:n_1/n_0 ,1: 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663e9276-1906-436c-a727-a386a2e1d891",
   "metadata": {},
   "source": [
    "### U_A) DT base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2eaa28f8-e818-4208-a500-2d62ad438368",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.6284002491249924\n",
      "Test Error = 0.3716\n",
      "----------------------------\n",
      "Auc_: 0.579449933011744 \n",
      " ----------------------\n",
      "Precision for class  1 : 83.43%\n",
      "Recall for class  1 : 64.57%\n",
      "avg_F1-Score for class  1 : 72.80%\n",
      "----------------------------\n",
      "Precision for class  2 : 32.46%\n",
      "Recall for class  2 : 57.04%\n",
      "avg_F1-Score for class  2 : 41.37%\n",
      "----------------------------\n",
      "----------------------\n",
      "avg_F1-Score: 57.09%\n",
      "----------------------\n",
      "[[56690. 31106.]\n",
      " [11256. 14947.]]\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(labelCol=\"Working_Weekend\", featuresCol=\"features\")\n",
    "\n",
    "dt = dt.fit(train_und)\n",
    "predictions = dt.transform(testData)\n",
    "print_metrics_and_cf(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "66a91677-8022-47d3-a6e8-e7855573f6a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(24, {0: 0.0384, 1: 0.0099, 3: 0.1458, 4: 0.0026, 7: 0.0024, 22: 0.5192, 23: 0.2816})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.featureImportances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5455bb5f-0e1d-464f-8a11-27372ffe49dc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### U_A)_a) DT tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "89a21b43-4aaa-4eaa-bcae-082d00a79bc8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.6443038974026088\n",
      "Test Error = 0.355696\n",
      "----------------------------\n",
      "Auc_: 0.6147521938160576 \n",
      " ----------------------\n",
      "Precision for class  1 : 87.17%\n",
      "Recall for class  1 : 63.10%\n",
      "avg_F1-Score for class  1 : 73.21%\n",
      "----------------------------\n",
      "Precision for class  2 : 35.78%\n",
      "Recall for class  2 : 68.88%\n",
      "avg_F1-Score for class  2 : 47.10%\n",
      "----------------------------\n",
      "----------------------\n",
      "avg_F1-Score: 60.15%\n",
      "----------------------\n",
      "[[55401. 32395.]\n",
      " [ 8154. 18049.]]\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier, RandomForestClassifier\n",
    "\n",
    "dt_param1 = DecisionTreeClassifier(labelCol=\"Working_Weekend\", featuresCol=\"features\",\n",
    "                                  maxDepth=20,\n",
    "                                  maxBins=32)\n",
    "                                   \n",
    "\n",
    "dt_param1 = dt_param1.fit(train_und)\n",
    "predictions_param1 = dt_param1.transform(testData)\n",
    "print_metrics_and_cf(predictions_param1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "09e9cb3f-6fbe-4cce-8109-871b6503eb60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(24, {0: 0.0239, 1: 0.1377, 2: 0.1139, 3: 0.1143, 4: 0.1187, 5: 0.1118, 6: 0.0354, 7: 0.0756, 8: 0.0137, 9: 0.0017, 10: 0.0001, 11: 0.0045, 12: 0.0004, 13: 0.0068, 14: 0.0003, 15: 0.0017, 16: 0.0, 17: 0.0025, 18: 0.0035, 19: 0.0, 20: 0.0038, 21: 0.0805, 22: 0.1096, 23: 0.0397})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_param1.featureImportances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16904f21-2a38-477f-9dd7-52983aa30d5d",
   "metadata": {},
   "source": [
    "### U_A)_b) DT tuning aumento numero profonditÃ  dell'albero e numero di bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dd254c38-f038-4ba6-80b5-63bb5ea3605d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.6755322415108904\n",
      "Test Error = 0.324468\n",
      "----------------------------\n",
      "Auc_: 0.6338804140436559 \n",
      " ----------------------\n",
      "Precision for class  1 : 88.15%\n",
      "Recall for class  1 : 66.86%\n",
      "avg_F1-Score for class  1 : 76.04%\n",
      "----------------------------\n",
      "Precision for class  2 : 38.63%\n",
      "Recall for class  2 : 69.89%\n",
      "avg_F1-Score for class  2 : 49.75%\n",
      "----------------------------\n",
      "----------------------\n",
      "avg_F1-Score: 62.90%\n",
      "----------------------\n",
      "[[58697. 29099.]\n",
      " [ 7890. 18313.]]\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier, RandomForestClassifier\n",
    "\n",
    "dt_param1 = DecisionTreeClassifier(labelCol=\"Working_Weekend\", featuresCol=\"features\",\n",
    "                                  maxDepth=30,\n",
    "                                  maxBins=64)\n",
    "                                   \n",
    "\n",
    "dt_param1 = dt_param1.fit(train_und)\n",
    "predictions_param1 = dt_param1.transform(testData)\n",
    "print_metrics_and_cf(predictions_param1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cdb8e5d2-c100-43a5-8ab4-395dd6f6466a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(24, {0: 0.0158, 1: 0.1797, 2: 0.1302, 3: 0.1274, 4: 0.1317, 5: 0.1243, 6: 0.0256, 7: 0.0708, 8: 0.0076, 9: 0.0015, 10: 0.0001, 11: 0.0049, 12: 0.0002, 13: 0.0069, 14: 0.0003, 15: 0.0013, 17: 0.0022, 18: 0.0044, 19: 0.0001, 20: 0.0044, 21: 0.0685, 22: 0.0699, 23: 0.0222})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_param1.featureImportances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc93e23-90fc-4f30-b020-c12316356493",
   "metadata": {},
   "source": [
    "### U_A)_c) DT tuning con pesi \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dc35171c-a4c4-4b62-be35-8ce803f9aa73",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|Working_Weekend|Count|\n",
      "+---------------+-----+\n",
      "|              1|61513|\n",
      "|              0|61572|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "g=train_und.groupBy('Working_Weekend').agg(F.count('Working_Weekend').alias('Count'))\n",
    "g.sort('Count').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8e0c07a8-c606-443a-802d-d4bb7c261e09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainingData_w = trainingData_w.withColumn(\"weights2\", when(trainingData[\"Working_Weekend\"] == 0, 1.0)\n",
    "    .when(trainingData[\"Working_Weekend\"] == 1, 122526 / 61237)  # Calcolo del peso per la classe 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "beffe57b-ccf3-4c0f-a4f3-aebc413e9d07",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.7873928718672971\n",
      "Test Error = 0.212607\n",
      "----------------------------\n",
      "Auc_: 0.7010528464525068 \n",
      " ----------------------\n",
      "Precision for class  1 : 86.59%\n",
      "Recall for class  1 : 85.66%\n",
      "avg_F1-Score for class  1 : 86.12%\n",
      "----------------------------\n",
      "Precision for class  2 : 53.62%\n",
      "Recall for class  2 : 55.55%\n",
      "avg_F1-Score for class  2 : 54.57%\n",
      "----------------------------\n",
      "----------------------\n",
      "avg_F1-Score: 70.34%\n",
      "----------------------\n",
      "[[75207. 12589.]\n",
      " [11648. 14555.]]\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier, RandomForestClassifier\n",
    "\n",
    "dt_param2 = DecisionTreeClassifier(labelCol=\"Working_Weekend\", featuresCol=\"features\",\n",
    "                                  maxDepth=30,\n",
    "                                  maxBins=32,\n",
    "                                weightCol='weights2'  \n",
    "                                  )\n",
    "                                   \n",
    "\n",
    "dt_param2 = dt_param2.fit(trainingData_w)\n",
    "predictions_param2 = dt_param2.transform(testData)\n",
    "print_metrics_and_cf(predictions_param2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1b476de1-c65d-4a8e-ad7b-e648908acc38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(24, {0: 0.0116, 1: 0.1459, 2: 0.1259, 3: 0.1172, 4: 0.1398, 5: 0.1274, 6: 0.0325, 7: 0.0903, 8: 0.0083, 9: 0.0014, 10: 0.0003, 11: 0.0055, 12: 0.0004, 13: 0.0085, 14: 0.0004, 15: 0.0014, 17: 0.0028, 18: 0.005, 19: 0.0002, 20: 0.0048, 21: 0.0736, 22: 0.0739, 23: 0.023})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_param2.featureImportances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4c8d13-389a-4510-a7f6-7920b0f61cc6",
   "metadata": {},
   "source": [
    "## il miglior DT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372a5b00-2361-4d69-a5b8-ab84d08bf9d5",
   "metadata": {},
   "source": [
    "### U_B) Random Forest Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3038d9f0-773e-4282-872c-ad53da4d6332",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.658768936569619\n",
      "Test Error = 0.341231\n",
      "----------------------------\n",
      "Auc_: 0.573835071731319 \n",
      " ----------------------\n",
      "Precision for class  1 : 81.83%\n",
      "Recall for class  1 : 71.58%\n",
      "avg_F1-Score for class  1 : 76.37%\n",
      "----------------------------\n",
      "Precision for class  2 : 32.93%\n",
      "Recall for class  2 : 46.75%\n",
      "avg_F1-Score for class  2 : 38.65%\n",
      "----------------------------\n",
      "----------------------\n",
      "avg_F1-Score: 57.51%\n",
      "----------------------\n",
      "[[62848. 24948.]\n",
      " [13952. 12251.]]\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "# Train a RandomForest model.\n",
    "rf = RandomForestClassifier(labelCol=\"Working_Weekend\", featuresCol=\"features\", numTrees=10)\n",
    "\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "model = rf.fit(train_und)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "print_metrics_and_cf(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "098a2ef0-36f3-42ae-81d1-ffccf2c874a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(24, {0: 0.0598, 1: 0.0149, 2: 0.0099, 3: 0.1754, 4: 0.0055, 5: 0.0094, 6: 0.0086, 7: 0.0091, 8: 0.0249, 11: 0.0001, 13: 0.0002, 16: 0.0, 17: 0.0001, 18: 0.0001, 20: 0.0001, 21: 0.0206, 22: 0.4004, 23: 0.2607})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.featureImportances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878a0d19-a61d-4fad-aa14-4b9e6f713d14",
   "metadata": {},
   "source": [
    "### U_B)_a) Ranodom Forest Tuning aumento max depth e numero di alberi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "94feb369-07c0-4bae-b7a2-19ca99d4e2db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.6794445565311976\n",
      "Test Error = 0.320555\n",
      "----------------------------\n",
      "Auc_: 0.6455109670992571 \n",
      " ----------------------\n",
      "Precision for class  1 : 89.59%\n",
      "Recall for class  1 : 66.05%\n",
      "avg_F1-Score for class  1 : 76.04%\n",
      "----------------------------\n",
      "Precision for class  2 : 39.51%\n",
      "Recall for class  2 : 74.30%\n",
      "avg_F1-Score for class  2 : 51.59%\n",
      "----------------------------\n",
      "----------------------\n",
      "avg_F1-Score: 63.81%\n",
      "----------------------\n",
      "[[57988. 29808.]\n",
      " [ 6735. 19468.]]\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(labelCol=\"Working_Weekend\", featuresCol=\"features\", numTrees=20,maxDepth=20, seed=10\n",
    "                           )\n",
    "# Train model.  This also runs the indexers.\n",
    "model = rf.fit(train_und)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "print_metrics_and_cf(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bd0eb283-d427-4840-b6d9-31e0bc5eee88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(24, {0: 0.0286, 1: 0.0989, 2: 0.0943, 3: 0.1136, 4: 0.0982, 5: 0.1005, 6: 0.041, 7: 0.0736, 8: 0.0183, 9: 0.0026, 10: 0.0002, 11: 0.0067, 12: 0.0006, 13: 0.0096, 14: 0.0005, 15: 0.0024, 16: 0.0, 17: 0.0042, 18: 0.006, 19: 0.0004, 20: 0.0076, 21: 0.0885, 22: 0.1654, 23: 0.038})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.featureImportances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f258859-fb25-4ab9-aba4-ef5af536b335",
   "metadata": {},
   "source": [
    "### U_C) NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "472ed4d6-502a-41a3-8c4b-bae195f82da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------+\n",
      "|Working_Weekend| Count|\n",
      "+---------------+------+\n",
      "|              0|205787|\n",
      "|              1| 61251|\n",
      "+---------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "g=trainingData_nn.groupBy('Working_Weekend').agg(F.count('Working_Weekend').alias('Count'))\n",
    "g=g.sort('Working_Weekend')\n",
    "g.show()\n",
    "\n",
    "g_p=g.toPandas()\n",
    "n_0=g_p.iloc[0,1]\n",
    "n_1=g_p.iloc[1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "75479f01-d629-4808-8857-9ae79674091a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_und_nn = trainingData_nn.sampleBy('Working_Weekend', fractions={0:n_1/n_0 ,1: 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "202314c7-a0ff-4cd0-85b5-b8a31a6c7fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.6377066233096443\n",
      "Test Error = 0.362293\n",
      "----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Davide\\anaconda3\\envs\\pyspark_env\\Lib\\site-packages\\pyspark\\sql\\context.py:158: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auc_: 0.5483206328588364 \n",
      " ----------------------\n",
      "Precision for class  1 : 79.98%\n",
      "Recall for class  1 : 70.52%\n",
      "avg_F1-Score for class  1 : 74.95%\n",
      "----------------------------\n",
      "Precision for class  2 : 29.68%\n",
      "Recall for class  2 : 41.35%\n",
      "avg_F1-Score for class  2 : 34.56%\n",
      "----------------------------\n",
      "----------------------\n",
      "avg_F1-Score: 54.76%\n",
      "----------------------\n",
      "[[62009. 25925.]\n",
      " [15521. 10944.]]\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "layers = [len(num_col),4]\n",
    "\n",
    "# Create the Multilayer Perceptron Classifier and set its parameters\n",
    "trainer = MultilayerPerceptronClassifier(\n",
    "    layers=layers,\n",
    "    labelCol=\"Working_Weekend\",\n",
    "    featuresCol=\"features\",\n",
    "    maxIter=100,  \n",
    "    tol=1e-6,\n",
    "    seed=None,\n",
    "    blockSize=32,\n",
    "    stepSize=0.03,  \n",
    "    solver=\"l-bfgs\",\n",
    "    initialWeights=None,\n",
    "    probabilityCol=\"probability\",\n",
    "    rawPredictionCol=\"rawPrediction\"\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model = trainer.fit(train_und_nn)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions= model.transform(testData_nn)\n",
    "print_metrics_and_cf(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6194af83-f444-4ae7-937d-89f5d8523543",
   "metadata": {},
   "source": [
    "### OVERSAMPLING MINORITY CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "620fd96e-9a4d-4c62-9c82-ac5834126ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "under_1=trainingData.filter(col('Working_Weekend') == 1)\n",
    "under_1 = trainingData.sample(True, 0.3, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1c56127a-bddd-47a5-aa1e-c41bb2a2641d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80137"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "under_1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a4ada9cf-8667-4e97-ae91-312a42ac3670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.359732902319962\n",
      "0.359732902319962\n"
     ]
    }
   ],
   "source": [
    "oversample_1=trainingData.filter(col('Working_Weekend') == 1)\n",
    "num_campioni_da_generare = n_0\n",
    "num_campioni_da_generare=(num_campioni_da_generare-n_1)/n_1 #percentuale \n",
    "#print(num_campioni_da_generare)\n",
    "full_1=oversample_1\n",
    "#print(full.count())\n",
    "while num_campioni_da_generare>1:\n",
    "    oversample_1= oversample_1.unionAll(full_1)\n",
    "    num_campioni_da_generare=num_campioni_da_generare-1.0\n",
    "    print(num_campioni_da_generare)\n",
    "df_minority_oversampled = full_1.sample(True, num_campioni_da_generare, seed=42)\n",
    "\n",
    "oversample_1 = oversample_1.unionAll(df_minority_oversampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2dcc5b2d-c5ab-4acd-b934-99fca22c5b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_over=oversample_1.unionAll(trainingData.filter(col('Working_Weekend') == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a043596e-07ed-4146-bb4b-35d287e511c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "412576"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_over.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bae4d6db-4e56-46e2-8926-90ef4bc52396",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_over=df_over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9ade36ca-cb93-4682-b32f-3c41ba9a6c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------+\n",
      "|Working_Weekend|Working_Weekend|\n",
      "+---------------+---------------+\n",
      "|              0|         205925|\n",
      "|              1|         206651|\n",
      "+---------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "g=df_over.groupBy('Working_Weekend').agg(F.count('Working_Weekend').alias('Working_Weekend'))\n",
    "g=g.sort('Working_Weekend')\n",
    "g.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abf6047-56a0-4ff1-891f-5f32ff762c98",
   "metadata": {},
   "source": [
    "### O_A) DT Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a9f0d5a1-912d-44ba-9914-fdb4aa900384",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(labelCol=\"Working_Weekend\", featuresCol=\"features\")\n",
    "\n",
    "dt = dt.fit(train_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e402e15a-5026-44bd-a936-754d4dc632ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.6279704207931649\n",
      "Test Error = 0.37203\n",
      "----------------------------\n",
      "Auc_: 0.580012434171895 \n",
      " ----------------------\n",
      "Precision for class  1 : 83.51%\n",
      "Recall for class  1 : 64.41%\n",
      "avg_F1-Score for class  1 : 72.73%\n",
      "----------------------------\n",
      "Precision for class  2 : 32.49%\n",
      "Recall for class  2 : 57.39%\n",
      "avg_F1-Score for class  2 : 41.49%\n",
      "----------------------------\n",
      "----------------------\n",
      "avg_F1-Score: 57.11%\n",
      "----------------------\n",
      "[[56550. 31246.]\n",
      " [11165. 15038.]]\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "predictions = dt.transform(testData)\n",
    "print_metrics_and_cf(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8ce68f24-5c02-4544-8345-45d1b7306536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(24, {0: 0.0219, 1: 0.0198, 3: 0.1612, 4: 0.0035, 7: 0.0026, 18: 0.0004, 22: 0.5014, 23: 0.2891})"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.featureImportances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fe54c3-112e-454b-af0a-d4b888c2e14c",
   "metadata": {},
   "source": [
    "### O_A)_a) DT Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cefc96b9-df61-4e6a-b894-d70bd6522600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.6752339932806428\n",
      "Test Error = 0.324766\n",
      "----------------------------\n",
      "Auc_: 0.6282421611421578 \n",
      " ----------------------\n",
      "Precision for class  1 : 87.39%\n",
      "Recall for class  1 : 67.59%\n",
      "avg_F1-Score for class  1 : 76.22%\n",
      "----------------------------\n",
      "Precision for class  2 : 38.26%\n",
      "Recall for class  2 : 67.31%\n",
      "avg_F1-Score for class  2 : 48.79%\n",
      "----------------------------\n",
      "----------------------\n",
      "avg_F1-Score: 62.51%\n",
      "----------------------\n",
      "[[59339. 28457.]\n",
      " [ 8566. 17637.]]\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier, RandomForestClassifier\n",
    "\n",
    "dt_param1 = DecisionTreeClassifier(labelCol=\"Working_Weekend\", featuresCol=\"features\",\n",
    "                                  maxDepth=20,\n",
    "                                  maxBins=32)\n",
    "                                   \n",
    "\n",
    "dt_param1 = dt_param1.fit(train_over)\n",
    "predictions_param1 = dt_param1.transform(testData)\n",
    "print_metrics_and_cf(predictions_param1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "aa4f10a7-2b43-4ad4-a7ea-5dea37dc4351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(24, {0: 0.0215, 1: 0.1192, 2: 0.1087, 3: 0.1057, 4: 0.1257, 5: 0.1173, 6: 0.0388, 7: 0.0783, 8: 0.013, 9: 0.0014, 10: 0.0001, 11: 0.0041, 12: 0.0001, 13: 0.0052, 14: 0.0003, 15: 0.0016, 17: 0.0021, 18: 0.0036, 19: 0.0001, 20: 0.0036, 21: 0.089, 22: 0.1156, 23: 0.0451})"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_param1.featureImportances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c994d3ec-e72b-4344-ae6f-4820f6cdaf5e",
   "metadata": {},
   "source": [
    "### O_A)_b) DT Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a26aacbf-ec56-49c5-95ca-70bafc67c46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------+\n",
      "|Working_Weekend| Count|\n",
      "+---------------+------+\n",
      "|              0|205925|\n",
      "|              1|206651|\n",
      "+---------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "g=train_over.groupBy('Working_Weekend').agg(F.count('Working_Weekend').alias('Count'))\n",
    "g.sort('Count').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4d5c6125-5ea3-4df5-a6d5-05c9f703a6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|Working_Weekend|Count|\n",
      "+---------------+-----+\n",
      "|              1|26203|\n",
      "|              0|87796|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "g=testData.groupBy('Working_Weekend').agg(F.count('Working_Weekend').alias('Count'))\n",
    "g.sort('Count').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3584a5c6-ab9a-439c-ba92-4cf7a0f5188c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData_w = train_over.withColumn(\"weights2\", when(trainingData[\"Working_Weekend\"] == 0, 1.0)\n",
    "    .when(trainingData[\"Working_Weekend\"] == 1, 411881 / 205956)  # Calcolo del peso per la classe 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cc79fb34-9232-4345-8eec-2e10c6af8e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.7478223493188537\n",
      "Test Error = 0.252178\n",
      "----------------------------\n",
      "Auc_: 0.6676623259810995 \n",
      " ----------------------\n",
      "Precision for class  1 : 87.21%\n",
      "Recall for class  1 : 78.82%\n",
      "avg_F1-Score for class  1 : 82.80%\n",
      "----------------------------\n",
      "Precision for class  2 : 46.33%\n",
      "Recall for class  2 : 61.25%\n",
      "avg_F1-Score for class  2 : 52.75%\n",
      "----------------------------\n",
      "----------------------\n",
      "avg_F1-Score: 67.78%\n",
      "----------------------\n",
      "[[69201. 18595.]\n",
      " [10153. 16050.]]\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier, RandomForestClassifier\n",
    "\n",
    "dt_param2 = DecisionTreeClassifier(labelCol=\"Working_Weekend\", featuresCol=\"features\",\n",
    "                                  maxDepth=30,\n",
    "                                  maxBins=32,\n",
    "                                weightCol='weights2'  \n",
    "                                  )\n",
    "                                   \n",
    "\n",
    "dt_param2 = dt_param2.fit(trainingData_w)\n",
    "predictions_param2 = dt_param2.transform(testData)\n",
    "print_metrics_and_cf(predictions_param2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e10a779a-ca94-499f-9a47-a4447734b2ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(24, {0: 0.0204, 1: 0.1369, 2: 0.1226, 3: 0.117, 4: 0.1316, 5: 0.1268, 6: 0.0327, 7: 0.0885, 8: 0.008, 9: 0.0022, 10: 0.0002, 11: 0.0058, 12: 0.0003, 13: 0.0091, 14: 0.0004, 15: 0.0017, 17: 0.003, 18: 0.0054, 19: 0.0001, 20: 0.0063, 21: 0.078, 22: 0.0824, 23: 0.0206})"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_param2.featureImportances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02110477-b67e-4cd2-b425-cf5aca9876d6",
   "metadata": {},
   "source": [
    "### O_B) Ranodom Forest Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4106667f-878b-474f-9daa-febe700c8e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.5674435740664392\n",
      "Test Error = 0.432556\n",
      "----------------------------\n",
      "Auc_: 0.5656856532750435 \n",
      " ----------------------\n",
      "Precision for class  1 : 83.54%\n",
      "Recall for class  1 : 54.59%\n",
      "avg_F1-Score for class  1 : 66.03%\n",
      "----------------------------\n",
      "Precision for class  2 : 29.60%\n",
      "Recall for class  2 : 63.96%\n",
      "avg_F1-Score for class  2 : 40.47%\n",
      "----------------------------\n",
      "----------------------\n",
      "avg_F1-Score: 53.25%\n",
      "----------------------\n",
      "[[47928. 39868.]\n",
      " [ 9443. 16760.]]\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "# Train a RandomForest model.\n",
    "rf = RandomForestClassifier(labelCol=\"Working_Weekend\", featuresCol=\"features\", numTrees=10)\n",
    "\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "model = rf.fit(train_over)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "print_metrics_and_cf(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6635cdb5-36dc-4e6d-b60c-a2f776fa00d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(24, {0: 0.0521, 1: 0.0187, 2: 0.0074, 3: 0.1684, 4: 0.0047, 5: 0.006, 6: 0.007, 7: 0.0061, 8: 0.0211, 9: 0.0, 11: 0.0001, 17: 0.0001, 18: 0.0001, 20: 0.0, 21: 0.0221, 22: 0.4441, 23: 0.242})"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.featureImportances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1cd1f3-361f-4ff8-aacd-99d8a50df6ad",
   "metadata": {},
   "source": [
    "### O_C) NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f11e969e-2b86-45ff-a196-b6a38ad4b063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.359732902319962\n",
      "0.359732902319962\n"
     ]
    }
   ],
   "source": [
    "oversample_1_nn=trainingData_nn.filter(col('Working_Weekend') == 1)\n",
    "num_campioni_da_generare = n_0\n",
    "num_campioni_da_generare=(num_campioni_da_generare-n_1)/n_1 #percentuale \n",
    "#print(num_campioni_da_generare)\n",
    "full_1_nn=oversample_1_nn\n",
    "#print(full.count())\n",
    "while num_campioni_da_generare>1:\n",
    "    oversample_1_nn= oversample_1_nn.unionAll(full_1_nn)\n",
    "    num_campioni_da_generare=num_campioni_da_generare-1.0\n",
    "    print(num_campioni_da_generare)\n",
    "df_minority_oversampled_nn = full_1_nn.sample(True, num_campioni_da_generare, seed=42)\n",
    "\n",
    "oversample_1_nn = oversample_1_nn.unionAll(df_minority_oversampled_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c7b8a0ec-14eb-4bb4-a1cc-f5b2b767b455",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_over_nn=oversample_1_nn.unionAll(trainingData_nn.filter(col('Working_Weekend') == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b995283d-161b-42ea-a98e-29ca3b20826c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------+\n",
      "|Working_Weekend|Working_Weekend|\n",
      "+---------------+---------------+\n",
      "|              0|         205787|\n",
      "|              1|         205767|\n",
      "+---------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "g=df_over_nn.groupBy('Working_Weekend').agg(F.count('Working_Weekend').alias('Working_Weekend'))\n",
    "g=g.sort('Working_Weekend')\n",
    "g.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "47dc628a-bcf3-4b1d-bc1b-559b97c066a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_over_nn=df_over_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d8079720-79a3-4d64-aff6-3371154a5423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.6249267913181059\n",
      "Test Error = 0.375073\n",
      "----------------------------\n",
      "Auc_: 0.5468267565140011 \n",
      " ----------------------\n",
      "Precision for class  1 : 80.11%\n",
      "Recall for class  1 : 68.12%\n",
      "avg_F1-Score for class  1 : 73.63%\n",
      "----------------------------\n",
      "Precision for class  2 : 29.25%\n",
      "Recall for class  2 : 43.81%\n",
      "avg_F1-Score for class  2 : 35.08%\n",
      "----------------------------\n",
      "----------------------\n",
      "avg_F1-Score: 54.36%\n",
      "----------------------\n",
      "[[59897. 28037.]\n",
      " [14871. 11594.]]\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "layers = [len(num_col),4]\n",
    "\n",
    "# Create the Multilayer Perceptron Classifier and set its parameters\n",
    "trainer = MultilayerPerceptronClassifier(\n",
    "    layers=layers,\n",
    "    labelCol=\"Working_Weekend\",\n",
    "    featuresCol=\"features\",\n",
    "    maxIter=100,  \n",
    "    tol=1e-6,\n",
    "    seed=None,\n",
    "    blockSize=32,\n",
    "    stepSize=0.03,  \n",
    "    solver=\"l-bfgs\",\n",
    "    initialWeights=None,\n",
    "    probabilityCol=\"probability\",\n",
    "    rawPredictionCol=\"rawPrediction\"\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model = trainer.fit(train_over_nn)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions= model.transform(testData_nn)\n",
    "print_metrics_and_cf(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
