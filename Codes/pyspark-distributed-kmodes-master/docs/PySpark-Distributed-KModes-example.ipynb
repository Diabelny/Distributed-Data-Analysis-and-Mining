{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/thinkbig.png)\n",
    "# Demonstration of distributed K-Modes clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First load the library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark_kmodes import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create some sample data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the data set\n",
    "import numpy as np\n",
    "data = np.random.choice([\"a\", \"b\", \"c\"], (50000, 10))\n",
    "data2 = np.random.choice([\"e\", \"f\", \"g\"], (50000, 10))\n",
    "data = list(data) + list(data2)\n",
    "\n",
    "from random import shuffle\n",
    "shuffle(data)\n",
    "\n",
    "# Create a Spark RDD from our sample data and decrease partitions to max_partions\n",
    "max_partitions = 32\n",
    "\n",
    "rdd = sc.parallelize(data)\n",
    "rdd = rdd.coalesce(max_partitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify 2 cluster centers and a maximum of 10 iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_clusters = 2\n",
    "max_iter = 10\n",
    "\n",
    "method = EnsembleKModes(n_clusters, max_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model using PySpark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  0\n",
      "Iteration  1\n",
      "Iteration  2\n",
      "Init: initializing centroids\n",
      "Init: initializing clusters\n",
      "Starting iterations...\n",
      "Run 1, iteration: 1/100, moves: 0, cost: 32.0\n",
      "Avg cost/partition: 4.0\n",
      "Final centroids:\n",
      "['a' 'a' 'a' 'a' 'b' 'a' 'b' 'c' 'a' 'c']\n",
      "['f' 'f' 'f' 'f' 'g' 'e' 'g' 'f' 'g' 'g']\n"
     ]
    }
   ],
   "source": [
    "model = method.fit(rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(['a', 'a', 'a', 'a', 'b', 'a', 'b', 'c', 'a', 'c'], dtype=object), array(['f', 'f', 'f', 'f', 'g', 'e', 'g', 'f', 'g', 'g'], dtype=object)]\n",
      "6.649289999999995\n"
     ]
    }
   ],
   "source": [
    "print(model.clusters)\n",
    "print(method.mean_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((0, array(['f', 'g', 'f', 'e', 'f', 'g', 'e', 'e', 'g', 'f'], \n",
      "      dtype='<U1')), (0, 1)), ((1, array(['a', 'c', 'a', 'b', 'c', 'c', 'b', 'b', 'b', 'a'], \n",
      "      dtype='<U1')), (1, 0)), ((2, array(['f', 'f', 'f', 'g', 'f', 'f', 'f', 'e', 'e', 'g'], \n",
      "      dtype='<U1')), (2, 1)), ((3, array(['a', 'c', 'a', 'b', 'a', 'c', 'c', 'b', 'b', 'c'], \n",
      "      dtype='<U1')), (3, 0)), ((4, array(['a', 'b', 'b', 'b', 'c', 'b', 'a', 'a', 'c', 'c'], \n",
      "      dtype='<U1')), (4, 0)), ((5, array(['b', 'a', 'b', 'c', 'c', 'b', 'b', 'a', 'b', 'a'], \n",
      "      dtype='<U1')), (5, 0)), ((6, array(['f', 'e', 'e', 'f', 'f', 'f', 'f', 'f', 'f', 'g'], \n",
      "      dtype='<U1')), (6, 1)), ((7, array(['g', 'e', 'g', 'f', 'f', 'f', 'e', 'g', 'f', 'e'], \n",
      "      dtype='<U1')), (7, 1)), ((8, array(['c', 'a', 'a', 'a', 'c', 'c', 'c', 'c', 'b', 'c'], \n",
      "      dtype='<U1')), (8, 0)), ((9, array(['g', 'f', 'e', 'g', 'g', 'f', 'g', 'g', 'f', 'f'], \n",
      "      dtype='<U1')), (9, 1))]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 0, 1, 0, 0]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = method.predictions\n",
    "datapoints = method.indexed_rdd\n",
    "combined = datapoints.zip(predictions)\n",
    "print(combined.take(10))\n",
    "\n",
    "model.predict(rdd).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 0, 0, 1, 0, 0, 1, 0]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(sc.parallelize(['e', 'e', 'f', 'e', 'e', 'f', 'g', 'e', 'f', 'e'])).collect()  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Spark 1.6.1)",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
